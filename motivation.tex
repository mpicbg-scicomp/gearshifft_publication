A FFT is a fast implementation of the discrete Fourier transform which is a standard text-book mathematical procedure. The forward transform is a mapping from an array $x$ of $n$ complex numbers in the time domain to an array $X$ of $n$ complex numbers in the frequency domain (also referred to as Fourier domain):

\begin{equation}
  \label{eq:dft}
  X[k] = \sum_{j=0}^{n-1} x[j]e^{\frac{-2\pi\sqrt{-1}jk}{n}}
\end{equation}

with $k$ being an integer index within $0 \le k < n$. This operation was found to be computable in $\mathcal{O}(n \log n)$ complexity by Cooley-Turkey \cite{cooley1965algorithm}, which in turn rediscovered findings by Gauss \cite{gauss}. The basis the Cooley-Turkey approach is the observation that, given the factorization of $n=n_1n_2$, the  DFT of size $n$ can be rewritten by smaller DFTs of size $n_1$ and $n_2$. Given the aforementioned indices $j=j_1n_2 + j_2$ and $k=k_1+k_2n_1$, \cref{eq:dft} can be re-expressed as:

\begin{equation}
  \label{eq:cooley-turkey}
  X[k_1 + k_2n_1] = \sum_{j_2=0}^{n_2-1} \left( \left( \sum_{j_1=0}^{n_1-1} x[j_1n_2 + j_2] e^{\frac{-2\pi\sqrt{-1}j_1k_1}{n_1}} \right) e^{\frac{-2\pi\sqrt{-1}j_2k_1}{n}} \right) e^{\frac{-2\pi\sqrt{-1}j_2k_2}{n_2}}
\end{equation}

\cref{eq:cooley-turkey} describes a decomposition that can be performed recursively \cite{FFTW05}. Here, $n_1$ is denoted \emph{radix} as it refers to $n_1$ transforms of size $n_2$. These smaller transforms are combined by a \emph{butterfly} graph with $n_2$ DFTs of size $n_1$ on the outputs of the corresponding sub-transforms. At this point, it should be noted that according to \cref{eq:cooley-turkey}, the operation will yield an algorithmic complexity, i.e. a ratio of compute operations per accessed byte of memory, below two. This can be observed as the inner-most sum $\sum_{j_1=0}^{n_1-1} x[j_1n_2 + j_2] e^{\frac{-2\pi\sqrt{-1}j_1k_1}{n_1}}$ yields at most one multiplication and one addition per read element of $x$. As the outer sum (over $j_2$) only implies an offset to this operation (see $x[j_1n_2 + j_2]$), the arithmetic complexity is increased by two. Writing the result back to $X$ will produce an access to memory. So, for every output element we have $4$ compute operations and $2$ memory operations, which gives an upper bound of the arithmetic complexity of $2$. If we consider the Cooley-Turkey to have a structure that represents most other FFT implementations, we can conclude that FFTs are mostly of low arithmetic complexity and hence will be memory bound for large input data and compute bound as soon as they can be operated on within the cache hierarchy.

Radix-2 DFTs ($n$ being a power of two) are mostly implemented with the Cooley-Tukey algorithm \cite{cooley1965algorithm}. Stockham's formulations of the FFT can be applied \cite{FFTW05} to avoid incoherent memory accesses. Arbitrary and mixed radices are more complicated and can be tackled with the prime-factorization or Chirp Z-transform implemented by the Bluestein's algorithm \cite{bluestein}. Due to these differences among the three major implementation schemas, we classify the FFT extent shapes of our benchmark data into \emph{powerof2} (radix-2), \emph{radix357} (mixed-radix of primes 3, 5 and 7) and into \emph{oddshape} (arbitrary radices excluding the previous cases). 

Given the multitude of mathematical formulations and the heterogenity of hardware, our aim is to provide a set of simple commands within \gearshifft{} to run benchmarks against a community's desired FFT implementation and compare to others in an unbiased way. Further, modern GPU based FFT libraries are updated regularly: cuFFT sees around 1-2 release updates per year with every CUDA version, clFFT was updated six times during the course of 2016). Library vendors need to assess if the most recent version of their used FFT implementation yields the performance profits to justify updating their package's dependencies. Further more, new users of FFT libraries are often overwhelmed by the number of different FFT implementations and their variations in use. 

Thus, \gearshifft{} approaches the challenge of benchmarking a variety of FFT libraries from a user perspective. This means, that the following parameters shall be easy to study:

\begin{itemize}
\item FFT dimension and radix-type (e.g. $32{\times}32{\times}32$ as radix-2 3D FFT)
\item transform kinds, i.e. real-to-complex or complex-to-complex transforms
\item precision, i.e. 32-bit or 64-bit IEEE floating point number representation
\item memory mode (``placeness'')
  \begin{itemize}
  \item \emph{in-place}: the input data structure is used for storing the output data (low memory footprint and low bandwidth are to be expected)
  \item \emph{out-of-place}:  where the transformed input is written to a different memory location than where the input resides (high memory footprint and high bandwidth are to be expected)
  \end{itemize}
\item transform direction, i.e. forward (from discrete space to frequency space) or backward (from frequency space to discrete space)
\end{itemize}
 
Last but not least, \gearshifft{} was primarily designed to study and compare the performance of GPU accelerators running FFT implementations versus classical CPU based FFT libraries. It's design has become more capable than this, which we try to demonstrate with this study.   
