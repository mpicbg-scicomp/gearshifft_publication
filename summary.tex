% - what can we conclude and recommend by the results?
% - what remains as part of future work?
% - issues?
% - encourage community-driven benchmark?
% - acknowledgements

With this paper, we present \gearshifft{} to the HPC community and other performance enthusiasts as an open-source and free FFT benchmark suite for heterogeneous platforms. \gearshifft{} is a C++14 modular benchmark code that allows to perform forward and backward FFT transforms on various types of input data (both in shape, memory organization, precision and data type). \gearshifft{}'s design offers an extensible architecture to accomodate FFT packages with low overhead. The hallmark of \gearshifft{} is to produce reliable benchmark data that can easily be consumed by external software for visualisation and for easier reproducibility. By these design choices, we hope that \gearshifft{} appeals to both FFT practitioners, FFT library developers and HPC admins or integrators for a wide range of use cases. To show case these capabilities, we presented a first study of common urban myths for using 3 state of the art FFT libraries, fftw, clfft and cufft. We were able to show that the time consumed for the creation of fftw plans can have non-negligible contributions to the time to solution which users of fftw should be aware of. Futhermore, we compared the performance of CPU based implementations Haswell Xeon CPUs to state-of-the-art Pascal generation Nvidia GPUs. We were able to show that for input signal sizes of less than \SI{1}{\mebi\byte}, the CPU implementation is superior whereas for larger input data size the GPU offers better turn-around. The difference between runtimes of power-of-2, {\tt radix357} and power-of-19 shaped input data was demonstrated to be negligible for fftw and non-negligible for cufft transforms used in this study. We were also able to identify runtime differences when using complex versus real arrays and when comparing double versus single precision data types.     

\paragraph{Acknowledgements.} The work of Matthias Werner was funded by Nvidia through the GPU Center of Excellence (GCOE) at the Center for Information Services and High Performance Computing (ZIH), TU Dresden, where the K20Xm and K80 GPU cluster Taurus was used. We would like to thank the Helmholtz-Zentrum Dresden-Rossendorf for providing the infrastructure to host the Nvidia Tesla P100 (provided by Nvidia for the GCOE) in the Hypnos HPC cluster. We would also like to thank the Max Planck Institute of Molecular Cell Biology and Genetics for supporting this publication by providing computing infrastructure and service staff working time.
