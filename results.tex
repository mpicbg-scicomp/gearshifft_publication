\subsection{Experimental Environment}
\label{ssec:env}

This section will discuss the results obtained with \gearshifft{} \texttt{v0.2.0} on various hardware in order to showcase the capabilities of \gearshifft{}. Based on the applications in \cite{preibisch2014efficient, schmid2015real}, 3D real-to-complex FFTs with contiguous single-precision input data are chosen for the experiments. If not stated, this is the transform type assumed for all illustrations hereafter.
%
Expeditions into other use cases will be made where appropriate. The curious reader may rest assured that a more comprehensive study is possible with \gearshifft{}, however the mere multiplicity of all possible combinations and use cases of FFT render it neither feasible nor practical to discuss all of them here.

This study concentrates on three modern and current FFT implementations available free of charge: \fftw{} ($3.3.6$pl1, on x86 CPUs), \cufft{} ($8.0.44$, on \nvidia{} GPUs) and \clfft{} ($2.12.2$, on x86 CPUs or \nvidia{} GPUs). This is considered as the natural starting point of developers beyond possible domain specific implementations.
It should be noted, that this will infer not only a study in terms of hardware performance, but also how well the APIs designed by the authors of \fftw{}, \clfft{} and \cufft{} can be used in practice.

The results presented in the following sections were collected on three hardware installations:
%
\begin{table}[tbp]
  \centering
  \caption{Benchmark Hardware}
  \label{tab:hardware}
  \begin{tabular}{lllll}
    \toprule
                        & \multicolumn{2}{c}{\textbf{Taurus}}           & \textbf{Hypnos}           & \textbf{Islay}                                  \\
                        & \multicolumn{2}{c}{HPC Cluster \cite{taurus}} & HPC Cluster \cite{hypnos} & Workstation                                     \\
    \midrule
    \textbf{CPU family} & Haswell Xeon                                  & Sandybridge Xeon          & Haswell Xeon           & Haswell Xeon           \\
    \textbf{CPU model } & $2{\times}$ E5-2680 v3                        & $2{\times}$ E5-2450       & $2{\times}$ E5-2603 v3 & $2{\times}$ E5-2640 v3 \\
    \textbf{RAM       } & \SI{64}{\gibi\byte}                           & \SI{48}{\gibi\byte}       & \SI{64}{\gibi\byte}    & \SI{64}{\gibi\byte}    \\
%    \textbf{GPU family} & Tesla                                         & Tesla                     & Tesla                  & GeForce                \\
    \textbf{GPU} {\scriptsize{(PCIe3.0)}} & 4x K80                                        & 2x K20x                   & 1x P100                & 1x GTX 1080            \\
    \textbf{GPU memory} & 4x \SI{12}{\gibi\byte}                        & \SI{6}{\gibi\byte}        & \SI{16}{\gibi\byte}    & \SI{8}{\gibi\byte}     \\
    \textbf{GPU driver} & $367.48$                                      & $367.48$                  & $367.48$               & $367.57$               \\
    \textbf{OS}         & RHEL $6.8$                                    & RHEL $6.8$                & Ubuntu $14.04.3$       & CentOS $7.2$           \\
    \bottomrule
  \end{tabular}
\end{table}
%
All systems presented in \cref{tab:hardware} will be used for the benchmarks in this section. Access was performed via an \texttt{ssh} session without running a graphical user interface on the target system. All measurements used the GNU compiler collection (GCC) version 5.3.0 as the underlying compiler. All used GPU implementations on \nvidia{} hardware interfaced with the proprietary driver and used the infrastructure provided by CUDA $8.0.44$ if not stated otherwise. 
After a warmup step a benchmark is executed ten times. From this, the arithmetic mean and sample standard deviations are used for most of the figures. 

%TODO: why maximum size of transforms?

\subsection{Overhead of \gearshifft{}}
\gearshifft{} is designed to be a lightweight framework with a thin wrapper for the FFT clients, where the interface between back-end and front-end is resolved at compile-time. Performance indicators of each benchmark are collected and buffered to be processed after the last benchmark finished. For validation purposes, a \cufft{} standalone code \cite{gearshifft_github} was created that either provides a timer harness like \gearshifft{} or only measures the time to solution of a straightforward implementation of a round-trip FFT transform. Both invoke a warmup step and ten repetitions of the whole round-trip FFT process. 
\cref{fig:verify_cufft} contains the distribution and the raw data of the time measurements. The diagrams indicate a constant, insignificant overhead effect of \gearshifft{}, where:
\begin{itemize}
\item \texttt{cufft} refers to the \gearshifft{} implementation,
\item \texttt{cufft-standalone} is the reference timing the operations from \cref{fig:framework},
\item \texttt{cufft-standalone-tts} is the reference only measuring time to solution.
\end{itemize}
\cref{fig:tts_verify_a} shows more noise as fluctuations have a higher impact on such small time scales. Here, \texttt{cufft-standalone-tts} shows slightly better performance as only one timer was used, otherwise reference and \gearshifft{} perform equally well. For better accuracy due to CPU affinity and cpu clock rate, benchmark data from different gearshifft launches should be evaluated in practise. 




\begin{figure}[!htb]
\vspace{-1em}
  \centering
  \includegraphics[width=\textwidth]{figures/results_validate_cufft_legend.pdf}\vspace{-1em}
  \subfloat[1024-point FFT]{\parbox[b]{0.45\textwidth}{%
    \includegraphics[width=0.45\textwidth]{figures/results_validate_cufft_a.pdf}\\
    \includegraphics[width=0.45\textwidth]{figures/results_validate_cufft_c.pdf}
    }\label{fig:tts_verify_a}}
  \hfill
  \subfloat[16777216-point FFT]{\parbox[b]{0.45\textwidth}{%
      \includegraphics[width=0.45\textwidth]{figures/results_validate_cufft_b.pdf}
      \includegraphics[width=0.45\textwidth]{figures/results_validate_cufft_d.pdf}
    }\label{fig:tts_verify_b}}
  \caption{Time-to-solution measured in \gearshifft{} and in a standalone \cufft{} application---multiple timer vs. one timer object (``tts'')---computing single-precision inplace real-to-complex round-trip FFTs on the K80 \cite{taurus}.}
  \label{fig:verify_cufft}
\end{figure}

\subsection{Time To Solution}
\label{ssec:tts}

The discussion begins with the classical use case for developers that might be accustomed to small size transforms. As such, an out-of-place transform with \texttt{powerof2} 3D signal shapes will be assumed. The memory volume required for this operation amounts to the real input array plus an equally shaped complex output array of the same precision.

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=\textwidth]{figures/results_tts_legend.pdf}\vspace{-1em}
  \subfloat[linear scale]{\includegraphics[width=0.45\textwidth]{figures/results_tts_a.pdf}\label{fig:tts_a}}
  \hfill
  \subfloat[log10-log2 scale]{\includegraphics[width=0.45\textwidth]{figures/results_tts_b.pdf}\label{fig:tts_b}}
  \caption{Time-to-solution for \texttt{powerof2} 3D single-precision real-to-complex out-of-place forward transforms using \fftw{} (\mc{FFTW_ESTIMATE}) and \cufft{}. \cref{fig:tts_b} shows the same data as \cref{fig:tts_a} but in a log10-log2 scale.}
  \label{fig:tts}
\end{figure}

\cref{fig:tts} reports a comparison of runtime results of \texttt{powerof2} single-precision 3D real-to-complex forward transforms from \fftw{} and \cufft{}. It is evident that given the largest device memory available of  \SI{16}{\gibi\byte}, the GPU data does not yield any points higher than \SI{8}{\gibi\byte}. The more recent GPU models supersede \fftw{} which used all $2{\times}12$ CPU Intel Haswell cores. Any judgment on the superiority of \cufft{} over \fftw{} can be considered premature at this point, as \fftw{} was used with the \mc{FFTW_ESTIMATE} planner flag.

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=\textwidth]{figures/results_plan_flags_legend.pdf}\vspace{-1em}
  \subfloat[time to solution]{\includegraphics[width=0.45\textwidth]{figures/results_plan_flags_a.pdf}\label{fig:plan_flags_a}}
  \hfill
  \subfloat[time for forward transform only]{\includegraphics[width=0.45\textwidth]{figures/results_plan_flags_b.pdf}\label{fig:plan_flags_b}}
  \caption{\fftw{} on Intel E5-2680v3 CPU with \mc{FFTW_ESTIMATE}, \mc{FFTW_MEASURE} and \mc{FFTW_WISDOM_ONLY} computing \texttt{powerof2} 3D single-precision real-to-complex inplace forward transforms. \cref{fig:plan_flags_a} reports the time to solution, whereas \cref{fig:plan_flags_b} shows the time spent for the execution of the forward transform only. Both figures use a log10-log2 scale.}
  \label{fig:fftw_plan_flags}
\end{figure}

\cref{fig:fftw_plan_flags} compares the time-to-solution to the actual time spent for the FFT operation itself. \mc{FFTW_MEASURE} imposes a total runtime penalty of 1 to 2 orders of magnitude with respect to \mc{FFTW_ESTIMATE}. It however offers superior performance considering FFT execution time compared to \mc{FFTW_ESTIMATE}. To compare \mc{FFTW_ESTIMATE} or \mc{FFTW_MEASURE} with plans using \emph{wisdom} (\mc{FFTW_WISDOM_ONLY} rigor), wisdom files are generated with the \mc{fftw_wisdom} binary. \mc{fftw_wisdom} precomputed plans for a canonical set of sizes (powers of two and ten up to $2^{20}$) in \mc{FFTW_PATIENT} mode, which in all took about one day on Taurus \cite{taurus} using (see \cite{fftw_manual} for command-line flag details):
\begin{lstlisting}[language=bash]
$ fftwf-wisdom -v -c -n -T 24 -o wisdomf
\end{lstlisting}%$%for emacs

As during plan creation, the \emph{wisdom} has to be loaded from disk only, the planning times for calling the planner with \mc{FFTW_WISDOM_ONLY} are drastically reduced. \cref{fig:plan_flags_b} shows that the user is rewarded by pure FFT runtimes of less than an order of magnitude for small signal sizes. Unexpectedly, the FFT runtimes become larger than those of \mc{FFTW_ESTIMATE} for input signal sizes of more than \SI{32}{\kibi\byte}, which apparently contradicts the \mc{FFTW_PATIENT} setting which should find better plans than \mc{FFTW_MEASURE}.
%
It must be emphasized that the planning times for \mc{FFTW_MEASURE} become prohibitively long and reach minutes for data sets in the gigabyte range. This is a well-known feature of \fftw{} as the authors note in \cite{FFTW05}:
%
\begin{quote}
  ``In performance critical applications, many transforms of the same
  size are typically required, and therefore a large one-time cost is
  usually acceptable.''
\end{quote}

\begin{figure}[!tbp]
  \centering
  \includegraphics[width=\textwidth]{figures/results_plan_time_legend.pdf}\vspace{-1em}
  \subfloat[3D transforms]{\includegraphics[width=0.45\textwidth]{figures/results_plan_time_a.pdf}\label{fig:plan_time_a}}
  \hfill
  \subfloat[1D transforms]{\includegraphics[width=0.45\textwidth]{figures/results_plan_time_b.pdf}\label{fig:plan_time_b}}
  \caption{Time-to-plan for \texttt{powerof2} single-precision inplace real-to-complex forward transforms using \fftw{} (Intel E5-2680v3 CPU), \cufft{} (K80 GPU) and \clfft{} (K80 GPU). \cref{fig:plan_time_a} reports the complete time to plan for 3D FFTs and \cref{fig:plan_time_b} for 1D FFTs. ``None'' refers to the planning with \cufft{} or \clfft{} as they do not support the plan rigor concept. Both figures use a log10-log2 scale.}
  \label{fig:plan_time}
\end{figure}

\gearshifft{} allows one to dissect this problem further and isolate the planning time only.
\cref{fig:plan_time} illustrates the problem to its full extent. \mc{FFTW_MEASURE} consumes up to 3--4 orders of magnitude more planning time than other plan-rigors and plans from GPU based libraries. The 3D planning is compared with its counterpart in 1D (see \cref{fig:plan_time_b}). It is important to note that \fftw{} planning in 1D appears to be very time consuming as the \mc{FFTW_MEASURE} curve is very steep compared to \cref{fig:plan_time_a}. At input sizes of \SI{128}{\mebi\byte} in 1D, the planning phase exceeds the duration of \SI{100}{\second}. The multi-threaded environment could be a problem for \fftw{} (compiled against OpenMP): when using 24 threads in \fftw{} the time to solution with \mc{FFTW_MEASURE} was up to $6{\times}$ slower than using 1 thread. Even worse, \mc{FFTW_PATIENT} was up to $50{\times}$ slower than in a single-thread environment. Unfortunately, the number of threads used for wisdoms, which usually run in \mc{FFTW_PATIENT} mode, must be equal to the ones used by the client later on.

In practice, this imposes a challenge on the client to the \fftw{} API. Not only is the time to solution affected by this behavior which is a crucial quantity in FFT-heavy applications. Moreover, in an HPC environment the runtime of applications needs to be known before executing them in order to allow efficient and rapid job placement on compute resources. From another perspective, this asserts a development pressure on the developer interfacing with \fftw{} as she has to create infrastructure in order to perform the planning of \fftw{} only once and reuse the resulting plan as much as possible. Furthermore, based on these observations of \cref{fig:fftw_plan_flags} and \cref{fig:plan_time} weighing plan time versus execution time, it becomes more and more unclear for a user of \fftw{} which plan rigor to use in general.

\subsection{Comparing CPU versus GPU runtimes}
\label{ssec:cpu_vs_gpu}

The last section finished by discussing a design artifact, that the \fftw{} authors introduced in their API and which other FFT libraries adopted. Another important and common question is whether GPU accelerated FFT implementations are really faster than their CPU equivalents. Although this question cannot be answered comprehensively in our study, there are several aspects to be explored. First of all, modern GPUs are connected via the PCIe bus to the host system in order to transfer data, receive instructions and to be supplied with power. This imposes a severe bottleneck to data transfer and is sometimes neglected during library design. Therefore, the time for data transfer needs to be accounted for or removed from the measurement. \gearshifft{}s results data model offers access to each individual step of a transformation, see \cref{fig:framework}. Hereby it is possible to isolate the runtime for the FFT transform.

\begin{figure}[!tbp]
  \centering
  \includegraphics[width=\textwidth]{figures/results_r2c_fwd_legend.pdf}\vspace{-1em}
  \subfloat[3D transforms]{\includegraphics[width=0.45\textwidth]{figures/results_r2c_fwd_a.pdf}\label{fig:r2c_fwd_a}}
  \hfill
  \subfloat[1D transforms]{\includegraphics[width=0.45\textwidth]{figures/results_r2c_fwd_b.pdf}\label{fig:r2c_fwd_b}}
  \caption{Time for computing \texttt{powerof2} out-of-place single-precision real-to-complex forward transforms for 3D and for 1D shapes. Both figures use a log10-versus-log2 scale. Curves on the Intel E5-2680v3 based node were obtained with \fftw{}, the data on \nvidia{} GPUs was obtained with \cufft{} and \clfft{}.}
  \label{fig:r2c_fwd}
\end{figure}

\cref{fig:r2c_fwd} shows the runtime spent for computing the forward FFT for real single precision input data. This illustration is a direct measure for the quality of the implementation and the hardware underneath. For the 3D case in \cref{fig:r2c_fwd_a} \fftw{} seems to provide compelling performance if the input data is not larger than \SI{1}{\mebi\byte} on a double socket Haswell Intel Xeon E5 CPU. Above this limit, the GPU implementations offer a clear advantage by up to one order of magnitude. The current Pascal generation GPUs used with \cufft{} provide the best performance, which does not come by surprise as both cards are equipped with GDDR5X or HBM2 memory which are clearly beneficial for an operation that yields rather low computational complexity such as the FFT. In the 1D case of \cref{fig:r2c_fwd_b}, the same observations must be made with even more certainty. The cross-over of \fftw{} and the GPU libraries occurs at an earlier point of \SI{64}{\kibi\byte}.  

Another observation in \cref{fig:r2c_fwd_a} is that the general structure of the runtime curves of GPU FFT implementations follows an inverse roofline curve \cite{williams2009roofline}. That is for input signals smaller than the roofline turning point at \SI{1}{\mebi\byte} the FFT implementation appears to be of constant cost, i.\,e. to be compute bound. Above the aforementioned threshold, the implementation appears to be memory bound and hence exposes a linear growth with growing input signals which corresponds to the $\mathcal{O}(n \log n)$ complexity observed in \cref{sec:intro} and validates the algorithmic complexity in \citep{williams2009roofline} as well. 

Finally, it is not to our surprise that the \clfft{} results reported in \cref{fig:r2c_fwd} cannot be considered optimal. As we executed \clfft{} on \nvidia{} hardware interfacing with the OpenCL runtime coming with CUDA and interfaced to the \nvidia{} proprietary driver, OpenCL performance can not be considered a first-class citizen in this environment. Only in \cref{fig:r2c_fwd_b}, the \clfft{} runtimes are below those of \fftw{}. These experiments should be repeated on AMD hardware where the OpenCL performance is expected to be better.
 
\subsection{Non-\texttt{powerof2} transforms}
\label{ssec:nonpowerof2}

It is often communicated, that input signals should be padded to \texttt{powerof2} shapes in order to achieve the highest possible performance. With \gearshifft{} the availability and quality of the common mathematical approaches across many FFT libraries can now be examined in detail. 
For the sake of brevity, only the results for \fftw{} (Intel E5-2680v3 CPU) and \cufft{} (P100) are presented here.

\begin{figure}[!tbp]
  \centering
  \includegraphics[width=\textwidth]{figures/results_non_power_of_2_legend.pdf}\vspace{-1em}
  \subfloat[Time for FFT]{\includegraphics[width=0.45\textwidth]{figures/results_non_power_of_2_a.pdf}\label{fig:non_power_of_2_a}}
  \hfill
  \subfloat[Time to Solution]{\includegraphics[width=0.45\textwidth]{figures/results_non_power_of_2_b_total.pdf}\label{fig:non_power_of_2_b}}
  \caption{\fftw{} and \clfft{} on Intel E5-2680v3 CPU with 24 threads versus \cufft{} on P100 GPU computing single-precision real-to-complex out-of-place forward transforms of 3D shapes. Both figures use a log10-versus-log2 scale.}
  \label{fig:non_power_of_2}
\end{figure}

\cref{fig:non_power_of_2} confirms that \texttt{powerof2} transforms are generally faster than \texttt{radix357} and \texttt{oddshape} transforms. Excluding the long planning time \fftw{} offers the fastest FFT runtime until the turning point at \SI{1}{\mebi\byte}, see \cref{fig:non_power_of_2_a}.
However, looking at time to solution in \cref{fig:non_power_of_2_b} \clfft{} on the CPU outperforms \fftw{} by \numrange{1}{2} orders of magnitude due to the long planning times of \fftw{}. At very small input signal sizes, \cufft{} lacks behind \clfft{} on the CPU until \SI{1}{\kibi\byte} for \texttt{powerof2} shapes, where \cufft{} offers superior or comparable runtimes thereafter.
\clfft{} only offers support for \texttt{powerof2} and \texttt{radix357} shape types but has almost the same performance for either. \cufft{} shows an FFT runtime difference of up to one order of magnitude on the P100 for large input signals (\cref{fig:non_power_of_2_a}) of \texttt{powerof2} and \texttt{oddshape} type, where the time to solution converges due to planning and transfer penalties (\cref{fig:non_power_of_2_a}).

For a large range of input signal sizes between \SIrange[exponent-base=2]{e-10}{e7}{\mebi\byte} a padding to \texttt{powerof2} might be justified when using \cufft{} if enough memory is available on the device. For \fftw{} non-\texttt{powerof2} signals can be padded at signal sizes above \SI[exponent-base=2]{e-3}{\mebi\byte} = \SI{128}{\kibi\byte}. \clfft{} on CPU is only a good choice, when short planning times are more important than transform runtime. \clfft{} provides similar performance on the P100 as on CPU, but it is not shown here.

\subsection{Data Types}
\label{ssec:data_types}

It is a common practice that complex-to-complex transforms are considered more performant than real-to-complex transforms. Therefore, in order to transform a real input array, a complex array is allocated and the real part of each datum is filled with the signal. The imaginary part of each datum is left at $0$.

% @todo remove K80 from plots
\begin{figure}[!tbp]
  \centering
  %\includegraphics[width=\textwidth]{figures/results_r2c_vs_c2c_legend.pdf}
  \subfloat[single precision]{\includegraphics[width=0.45\textwidth]{figures/results_r2c_vs_c2c_a.pdf}\label{fig:r2c_vs_c2c_a}}
  \hfill
  \subfloat[real-to-complex]{\includegraphics[width=0.45\textwidth]{figures/results_r2c_vs_c2c_b.pdf}\label{fig:r2c_vs_c2c_b}}
  \caption{Time for computing a forward FFT using 3D \texttt{powerof2} input signals using \fftw{} and \cufft{} on respective hardware versus the number of elements in the input signal. \cref{fig:r2c_vs_c2c_a} computes a real-to-complex transform and compares it to a complex-to-complex transform for single precision input data, whereas \cref{fig:r2c_vs_c2c_b} shows a real-to-complex transform for either single or double precision. Both figures use a log2-versus-log2 scale.}
  \label{fig:r2c_vs_c2c}
\end{figure}

\cref{fig:r2c_vs_c2c} restricts itself to larger signal sizes in order to aid the visualization. Note that in \cref{fig:r2c_vs_c2c_a}, a data point at the same number of elements of the input signal does have different size in memory. \fftw{} exposes a factor of 2 and more of runtime difference for signals larger than $2^{15}$ elements comparing real and complex input data types in \cref{fig:r2c_vs_c2c_a}. Below this threshold, the performance can be considered identical except for very small input signals although real FFTs always remain faster than complex ones. The situation is different for \cufft{}, where the overall difference is smaller in general. In the compute bound region of \cufft{} (below $2^{19}$ elements), complex transforms perform equally well than real transforms given the observed uncertainties. In the memory bound region (above $2^{19}$ elements), real transforms can be a factor of 2 ahead of complex ones which is clearly related to twice the memory accesses.    

If single-precision can be used instead of double-precision, then the possible performance gain can be estimated by \cref{fig:r2c_vs_c2c_b}. On the high grade server GPU, the \nvidia{} Tesla P100, the performance difference remains around $2{\times}$ in the memory bound region due to double the memory bandwidth required. The results for \fftw{} vary more around \numrange{1.5}{2.5} fold regressions between single and double precision inputs across a wider input signal range. 


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "gearshifft"
%%% End:
