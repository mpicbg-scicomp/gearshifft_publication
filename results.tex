Based on the experiences made for \cite{preibisch2014efficient, schmid2015real}, this section will discuss results obtained with gearshifft on various hardware in order to showcase the capabilities of \gearshifft{}. We will assume the motivation of a developer seeking to optimize the use of FFTs in the context of the aforementioned publications, i.e. 3D real-to-complex transforms with continguous single-precision input data. If not stated otherwise, this is the transform type assumed for all illustrations hereafter. 

Expeditions into other use cases will be made where appropriate. The curious reader may rest assured that a more comprehensive study is easily possible with \gearshifft{}, however the mere multiplicity of all possible combinations and use cases of FFT render it neither feasible nor practical to discuss 1D or 2D in a comprehensive fashion as well.

For this study, we will concentrate on three modern and current FFT implementations available free of charge: fftw (on x86 CPUs), cufft (on nVidia GPUs) and clfft (on x86 CPUs or nVidia GPUs). We consider this the natural starting point of developers beyond possible domain specific implementations. It should be noted, that this will infer not only a study in terms of hardware performance, but also how well the APIs designed by the authors of fftw, clFFT and cuFFT are documented, understood and used in practise. We consider both hardware and cognitive performance a virtue of almost equal importance.

\subsection{Experimental Environment}
\label{ssec:env}

The results presented in the following sections were collected on three systems:

\begin{itemize}
\item \emph{Taurus HPC cluster}\cite{taurus} running RHEL 7.2
  \begin{itemize}
  \item \emph{K80 node}: 2x Intel(R) Xeon(R) CPU E5-2680 v3 (12 cores) @ 2.50GHz, 64 GB RAM, 4x NVIDIA Tesla K80 (12 GB GDDR5 RAM) GPUs 
  \item \emph{K20X node}: 2x Intel(R) Xeon(R) CPU E5-2450 (8 cores) @ 2.10GHz, 48 GB RAM, 2x NVIDIA Tesla K20x (6 GB GDDR RAM) GPUs 
  \end{itemize}
\item \emph{Hypnos HPC cluster}\cite{hypnos} running Ubuntu 14.04.3:\newline
  2x Intel(R) Xeon(R) CPU E5-2603 v3 @ 1.60GHz, 64 GB RAM (2.67 GB per core), 1x NVIDIA Tesla P100 (16 GB HBM2 RAM) GPUs via PCIe 
\item  \emph{Dell workstation} running CentOS 7.2:\newline 
  2x Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60GHz, 64 GB RAM, 1x NVIDIA GeForce GTX 1080 (8 GB GDDR5X RAM)
\end{itemize}

As presented above, all used systems operated on a variant of 64-bit Linux and were accessed via an ssh session without running a graphical user interface of any kind. All measurements used the GNU compiler collection (GCC, \cite{stallman2001using}) version 5.3.0 as the underlying compiler if not stated otherwise. 

The FFT implementations evaluated were:

\begin{itemize}
\item fftw \cite{FFTW05}, version 3.3.5
\item cuFFT from CUDA 8.0.44
\item clFFT 2.12.2
\end{itemize}

All used nVidia GPU implementations interfaced with the proprietary driver provided by the vendor. 

In order to generate one data set, a set arrays of arbitrary shapes is provided to a specific FFT API. The configuration files thereof can be accessed via \cite{gearshifft_github}. The shape configurations are separated in groups: \texttt{powerof2} (all dimensions are powers of $2$), \texttt{radix357} (all dimenions are either powers of $3$, $5$ or $7$) and \texttt{oddshape} (all dimenions are powers of $19$ in order to emulated very uncommon signal sizes). These configurations were generated in order to probe the FFT implementations for a wide spectrum of possible applications.  

The FFT calls to benchmark are executed five times each. From this, the arithmetic mean and sample standard deviations are used for figures presented below. As the number of repititions is a configurable parameter of \gearshifft{}, we leave it to the user to produce a more comprehensive data set than used for this publication. We consider five repetitions enough at this point to show and discuss several aspects of performance and usability of \gearshifft{} and the FFT libraries under study.  

%TODO: why maximum size of transforms?

\subsection{Time To Solution}
\label{ssec:tts}

We begin the discussion with the classical use case for developers that might be accustomed to small size transforms. As such, an out-of-place transform with \texttt{powerof2} signal shapes will be assumed. The memory volume required for this operation amounts to the real input array plus an equally shaped complex output array.   

\begin{figure}[!tbp]
  \centering
  \def\svgwidth{0.95\columnwidth}
  \input{figures/results_figure1_legend.pdf_tex}
  \def\svgwidth{0.45\columnwidth}
  \subfloat[Fig A.]{\input{figures/results_figure1a.pdf_tex}\label{fig:f1a}}
  \hfill
  \def\svgwidth{0.45\columnwidth}
  \subfloat[Fig B.]{\input{figures/results_figure1b.pdf_tex}\label{fig:f1b}}
  \caption{Time-to-solution for power-of-2 3D single-precision real-to-complex forward transforms using fftw (\texttt{FFTW_ESTIMATE}) and cufft. \cref{fig:f1b} shows the same data as \cref{fig:f1a} but in a log10-log2 scale.}
  \label{fig:tts}
\end{figure}

\cref{fig:tts} reports a comparison of runtime results of power-of-2 single-precision real-to-complex forward transforms from fftw and cufft. It is evident that given the largest device memory available of $\unit[16]{GB}$, the GPU data does not yield any points higher than $\unit[8]{GB}$. Note that the total time reflects the time to set up a plan, allocate memory on device, perform the data transfer onto the device, execute the FFT, transfer the result back to the host and clean up the used plan and the allocated memory. \cref{fig:f1a} shows that the oldest GPU generation used in this comparison yields the slowest results for input signals in the order of $\unit[1-2]{GB}$. All other and more recent GPU models supersede fftw using all available cores in this node. Any judgement on the superiority of cufft over fftw can be considered premature at this point, as fftw was used with the \texttt{FFTW_ESTIMATE} planner flag.

\begin{figure}[!tbp]
  \centering
  \def\svgwidth{0.95\columnwidth}
  \input{figures/results_figure2_legend.pdf_tex}
  \def\svgwidth{0.45\columnwidth}
  \subfloat[Fig A.]{\input{figures/results_figure2a.pdf_tex}\label{fig:f2a}}
  \hfill
  \def\svgwidth{0.45\columnwidth}
  \subfloat[Fig B.]{\input{figures/results_figure2b.pdf_tex}\label{fig:f2b}}
  \caption{Time-to-solution for power-of-2 3D single-precision real-to-complex forward transforms using fftw (\texttt{FFTW_ESTIMATE}) and cufft. \cref{fig:f2a} report the complete time to solution, whereas \cref{fig:f2b} is limited to the time spent for the execution of the forward transform only. Both figures use a in a log10-log2 scale.}
  \label{fig:fftw_plan_flags}
\end{figure}

\cref{fig:fftw_plan_flags} compares the time-to-solution to the actual time spent for the FFT operation itself. This illustration makes the cost and the benefit of higher planning flags than \texttt{FFTW_ESTIMATE} obvious. Where \texttt{FFTW_MEASURE} imposes a runtime penalty of 1 to 2 orders of magnitude with respect to \texttt{FFTW_ESTIMATE}, it offers superior performance. The careful observer has noticed that the planning times for \texttt{FFTW_MEASURE} become prohibitively large for large input data sizes and reach minutes for data sets in the Gigabyte range. This is a well-known feature of fftw as the authors note in \cite{FFTW05}:

\begin{center}
  ``In performance critical applications, many transforms of the same
  size are typically required, and therefore a large one-time cost is
  usually acceptable.''
\end{center}
 
\gearshifft{} allows to disect this problem further and isolate the planning time only.

\begin{figure}[!tbp]
  \centering
  \def\svgwidth{0.95\columnwidth}
  \input{figures/results_figure3a.pdf_tex}}
  \caption{Time-to-plan for power-of-2 3D single-precision real-to-complex forward transforms using fftw, clfft and cufft. The figure uses a log10-log2 scale.}
  \label{fig:plan_time_only}
\end{figure}

\cref{fig:plan_time_only} illustrates the problem to it's full extent. \texttt{FFTW_MEASURE} consumes up to 3-4 orders of magnitude more time to produce a plan than a standard (GPU based libraries) or \texttt{FFTW_ESTIMATE} based planner call. We can only hypothesize that this is due to the fact that:

\begin{itemize}
\item the factorization of the radix found during planning takes a lot of time
\item the available parameter space of possible FFT implementations for a given radix factorisation becomes very large on the CPU
\item the optimisation to find the optimal implementation for a given radix is too time consuming 
\end{itemize}



% - analysis starting from 3D
% + inplace versus outplace
% + real vs complex-as-real
